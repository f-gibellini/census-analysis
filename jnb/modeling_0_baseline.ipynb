{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff03fdcf-4535-4d49-af58-5a4f3c0220bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if not os.path.abspath(os.pardir) in sys.path:\n",
    "    sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "from src.data_reader import read_data\n",
    "from src.features_processor import FeaturesProcessor\n",
    "from src.ml_experiments import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f89c52e-f7d5-4cc8-bcc3-53bbf087fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "MODELS_PATH = '../models/'\n",
    "\n",
    "TRAIN_FNAME = 'census_income_learn.csv'\n",
    "TEST_FNAME = 'census_income_test.csv'\n",
    "METADATA_FNAME = 'census_income_metadata.txt'\n",
    "MODELS_LOG_FNAME = 'experiments_log.csv'\n",
    "\n",
    "COLORS = ['#4DC9C3', '#221C35', '#FCCD20', '#20C3EF', '#00B257', '#FF7700']\n",
    "\n",
    "TARGET_NAME = 'income'\n",
    "IGNORE_FEATURES = ['instance_weight']\n",
    "\n",
    "N_JOBS = 6\n",
    "GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c902027-710b-416f-83c4-fc7ab5edb4ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read input data file ../data/census_income_learn.csv\n",
      "Read metadata file ../data/census_income_metadata.txt\n",
      "Couldnt parse line \n",
      "Mapped columns\n",
      "Read input data file ../data/census_income_test.csv\n",
      "Read metadata file ../data/census_income_metadata.txt\n",
      "Couldnt parse line \n",
      "Mapped columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((99762, 41), (199523, 41))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = read_data(DATA_PATH + TRAIN_FNAME, DATA_PATH + METADATA_FNAME, drop_cols = IGNORE_FEATURES)\n",
    "test_df = read_data(DATA_PATH + TEST_FNAME, DATA_PATH + METADATA_FNAME, drop_cols = IGNORE_FEATURES)\n",
    "\n",
    "test_df.shape, train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec151ad3-c034-434f-81e5-cdaef281d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from src.constants import *\n",
    "\n",
    "class FeaturesProcessor():\n",
    "\n",
    "    def __init__(self, cat_encoding = 'OHE', encoders = None, \n",
    "                 age_buckets = [(18,25),(25, 35),(35,45),(45,55),(55, 65),(65,120)],\n",
    "                 work_weeks_buckets = [(0,10), (10,30), (30,45), (45,52)],\n",
    "                 investment_buckets = [(0,2000), (2000, 10000), (10000,100000), (100000,10000000)],\n",
    "                 marital_status_mapping = {\n",
    "                    'widowed':0, 'divorced':0, 'never married':0, 'separated':0, 'married-spouse absent':0,\n",
    "                    'married-civilian spouse present':1, 'married-a f spouse present':1 },\n",
    "                 working_h_per_week = 40,\n",
    "                 scaling_values = {}\n",
    "                ):\n",
    "\n",
    "        self.cat_encoding = cat_encoding\n",
    "        self.encoders = encoders\n",
    "        self.age_buckets = age_buckets\n",
    "        self.work_weeks_buckets = work_weeks_buckets\n",
    "        self.investment_buckets = investment_buckets\n",
    "        self.marital_status_mapping = marital_status_mapping\n",
    "        self.working_h_per_week = working_h_per_week\n",
    "        self.scaling_values = scaling_values\n",
    "\n",
    "\n",
    "    def _make_encoder(self, df, col):\n",
    "\n",
    "        mode = self.cat_encoding\n",
    "        \n",
    "        if mode == 'OHE':\n",
    "            encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        else:\n",
    "            encoder = LabelEncoder()\n",
    "        \n",
    "        encoder.fit(df[[col]])\n",
    "        self.encoders[col] = encoder\n",
    "\n",
    "\n",
    "    \n",
    "    def make_calculated_features(self, df):\n",
    "\n",
    "        #discretize age\n",
    "        for ab in self.age_buckets:\n",
    "            df[f'age_bucket_{ab[0]}_{ab[1]}'] = df.age.apply(lambda x:1 if (x>=ab[0] and x<ab[1]) else (0)).astype(int)\n",
    "\n",
    "        for wb in self.work_weeks_buckets:\n",
    "            df[f'ww_bucket_{wb[0]}_{wb[1]}'] = df.weeks_worked_in_year.apply(lambda x:1 if (x>=wb[0] and x<wb[1]) else (0)).astype(int)\n",
    "\n",
    "        # calculate overall income from investments\n",
    "        df['investments_income'] = (df.capital_gains + df.dividends_from_stocks - df.capital_losses).astype(int)\n",
    "\n",
    "        # calculate discrete buckets for it\n",
    "        for wb in self.investment_buckets:\n",
    "            df[f'ww_bucket_{wb[0]}_{wb[1]}'] = df.investments_income.apply(lambda x:1 if (x>=wb[0] and x<wb[1]) else (0)).astype(int)\n",
    "\n",
    "        # simplify marital status\n",
    "        df['marital_status_simple'] = df.marital_stat.map(self.marital_status_mapping).astype(int)\n",
    "\n",
    "        # look for different type of employment and simplify\n",
    "        df['work_govm'] = df.class_of_worker.fillna('').apply(lambda x:1 if ('government' in x) else(0)).astype(int)\n",
    "        df['work_pvt'] = df.class_of_worker.fillna('').apply(lambda x:1 if ('private' in x) else(0)).astype(int)\n",
    "        df['work_self'] = df.class_of_worker.fillna('').apply(lambda x:1 if ('self-employed' in x) else(0)).astype(int)\n",
    "\n",
    "        # calculate yearly salary \n",
    "        df['tot_salary'] = (df.wage_per_hour*df.weeks_worked_in_year*self.working_h_per_week).astype(int)\n",
    "        \n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform_features(self, df, make_calculated = False, scale = True):\n",
    "\n",
    "        mode = self.cat_encoding\n",
    "\n",
    "        #if want to make calculated features\n",
    "        if make_calculated:\n",
    "            df = self.make_calculated_features(df)\n",
    "\n",
    "        if scale:\n",
    "            print('Scaling Numerical Features')\n",
    "            if  len(self.scaling_values) < 1:\n",
    "                print('Missing max values to scale in 0..1 range, will calculate from current data')\n",
    "\n",
    "        \n",
    "        if mode not in ['OHE', 'LE']:\n",
    "            print(f'Unsupported mode {mode} defaulting to One Hot Encoding')\n",
    "            mode = 'OHE'\n",
    "\n",
    "        if self.encoders is None:\n",
    "            print('Encoders not found, creating new encoders')\n",
    "            self.encoders = {}\n",
    "        else:\n",
    "            print('Encoders were found, using existing encoders')\n",
    "        \n",
    "        encoded_df = pd.DataFrame()\n",
    "        \n",
    "        for col in df.columns:\n",
    "            #if column is object (not numerical)\n",
    "            if (df[col].dtype == 'O') and (col != TARGET_NAME):\n",
    "\n",
    "                if not (col in self.encoders.keys()):\n",
    "                    self._make_encoder(df, col)\n",
    "                \n",
    "                encoder = self.encoders[col]\n",
    "\n",
    "                encoded_features = encoder.transform(df[[col]])\n",
    "                if mode == 'OHE':\n",
    "                    feat_names = [f\"{col}_{cat}\" for cat in encoder.categories_[0]]\n",
    "                else:\n",
    "                    feat_names = [col]\n",
    "                    \n",
    "                encoded_df = pd.concat([encoded_df,\n",
    "                                        pd.DataFrame(encoded_features, columns=feat_names)], axis=1)\n",
    "            elif (df[col].dtype != 'O') and (col != TARGET_NAME):\n",
    "                if scale:\n",
    "                    if not (col in self.scaling_values.keys()):\n",
    "                        self.scaling_values[col] = df[col].max()\n",
    "                    \n",
    "                    encoded_df[col] = df[col].clip(0,self.scaling_values[col]) /self.scaling_values[col]\n",
    "                    \n",
    "            \n",
    "            else: #target column\n",
    "                encoder = LabelEncoder()\n",
    "        \n",
    "                encoder.fit(df[[col]])\n",
    "                self.encoders[col] = encoder \n",
    "                encoded_df[col] = encoder.transform(df[[col]])\n",
    "                \n",
    "        return encoded_df, self.encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72024584-9085-4407-8b67-cb8ac1e475b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling Numerical Features\n",
      "Missing max values to scale in 0..1 range, will calculate from current data\n",
      "Encoders not found, creating new encoders\n",
      "Scaling Numerical Features\n",
      "Encoders were found, using existing encoders\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((179570, 404), (19953, 404), (99762, 404))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fprocessor = FeaturesProcessor('OHE')\n",
    "\n",
    "encoded_train_df, encoders = fprocessor.transform_features(train_df)\n",
    "encoded_test_df, encoders = fprocessor.transform_features(test_df)\n",
    "\n",
    "encoded_train_df, encoded_val_df = train_test_split(encoded_train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "encoded_train_df.shape, encoded_val_df.shape, encoded_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f99cf34-54e2-48b1-ba92-83acf2b30e6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0,class_weight=balanced,dual=False,fit_intercept=True,intercept_scaling=1,l1_ratio=None,max_iter=200,multi_class=deprecated,n_jobs=6,penalty=l2,random_state=32,solver=lbfgs,tol=0.0001,verbose=0,warm_start=False)\n",
      "[0.4274240940254652, 0.8927986906710311, 0.2809683234612413, '[[15939   131]\\n [ 2792  1091]]']\n",
      "\n",
      "LogisticRegression(C=1.0,class_weight=balanced,dual=False,fit_intercept=True,intercept_scaling=1,l1_ratio=None,max_iter=500,multi_class=deprecated,n_jobs=6,penalty=l2,random_state=32,solver=lbfgs,tol=0.0001,verbose=0,warm_start=False)\n",
      "[0.4274240940254652, 0.8927986906710311, 0.2809683234612413, '[[15939   131]\\n [ 2792  1091]]']\n",
      "\n",
      "LogisticRegression(C=1.0,class_weight=balanced,dual=False,fit_intercept=True,intercept_scaling=1,l1_ratio=None,max_iter=1000,multi_class=deprecated,n_jobs=6,penalty=l2,random_state=32,solver=lbfgs,tol=0.0001,verbose=0,warm_start=False)\n",
      "[0.4274240940254652, 0.8927986906710311, 0.2809683234612413, '[[15939   131]\\n [ 2792  1091]]']\n",
      "\n",
      "LogisticRegression(C=0.1,class_weight=balanced,dual=False,fit_intercept=True,intercept_scaling=1,l1_ratio=None,max_iter=200,multi_class=deprecated,n_jobs=6,penalty=l2,random_state=32,solver=lbfgs,tol=0.0001,verbose=0,warm_start=False)\n",
      "[0.4171779141104294, 0.8903436988543372, 0.27240861291937907, '[[15825   134]\\n [ 2906  1088]]']\n",
      "\n",
      "LogisticRegression(C=0.1,class_weight=balanced,dual=False,fit_intercept=True,intercept_scaling=1,l1_ratio=None,max_iter=500,multi_class=deprecated,n_jobs=6,penalty=l2,random_state=32,solver=lbfgs,tol=0.0001,verbose=0,warm_start=False)\n",
      "[0.4171779141104294, 0.8903436988543372, 0.27240861291937907, '[[15825   134]\\n [ 2906  1088]]']\n",
      "\n",
      "LogisticRegression(C=0.1,class_weight=balanced,dual=False,fit_intercept=True,intercept_scaling=1,l1_ratio=None,max_iter=1000,multi_class=deprecated,n_jobs=6,penalty=l2,random_state=32,solver=lbfgs,tol=0.0001,verbose=0,warm_start=False)\n",
      "[0.4171779141104294, 0.8903436988543372, 0.27240861291937907, '[[15825   134]\\n [ 2906  1088]]']\n",
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0,class_weight=balanced,criterion=gini,max_depth=4,max_features=None,max_leaf_nodes=None,min_impurity_decrease=0.0,min_samples_leaf=1,min_samples_split=2,min_weight_fraction_leaf=0.0,monotonic_cst=None,random_state=32,splitter=best)\n",
      "[0.34113816095518684, 0.900163666121113, 0.21044576238760282, '[[14604   122]\\n [ 4127  1100]]']\n",
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0,class_weight=balanced,criterion=gini,max_depth=7,max_features=None,max_leaf_nodes=None,min_impurity_decrease=0.0,min_samples_leaf=1,min_samples_split=2,min_weight_fraction_leaf=0.0,monotonic_cst=None,random_state=32,splitter=best)\n",
      "[0.4097444089456869, 0.839607201309329, 0.27099841521394613, '[[15971   196]\\n [ 2760  1026]]']\n",
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0,class_weight=balanced,criterion=gini,max_depth=10,max_features=None,max_leaf_nodes=None,min_impurity_decrease=0.0,min_samples_leaf=1,min_samples_split=2,min_weight_fraction_leaf=0.0,monotonic_cst=None,random_state=32,splitter=best)\n",
      "[0.4067013287117273, 0.8641571194762684, 0.2659279778393352, '[[15816   166]\\n [ 2915  1056]]']\n",
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0,class_weight=balanced,criterion=gini,max_depth=13,max_features=None,max_leaf_nodes=None,min_impurity_decrease=0.0,min_samples_leaf=1,min_samples_split=2,min_weight_fraction_leaf=0.0,monotonic_cst=None,random_state=32,splitter=best)\n",
      "[0.41349032129315505, 0.8477905073649754, 0.27342306677223543, '[[15978   186]\\n [ 2753  1036]]']\n",
      "\n",
      "RandomForestClassifier(bootstrap=True,ccp_alpha=0.0,class_weight=None,criterion=gini,max_depth=3,max_features=0.3,max_leaf_nodes=None,max_samples=None,min_impurity_decrease=0.0,min_samples_leaf=1,min_samples_split=2,min_weight_fraction_leaf=0.0,monotonic_cst=None,n_estimators=500,n_jobs=6,oob_score=False,random_state=32,verbose=0,warm_start=False)\n",
      "[0.2023121387283237, 0.11456628477905073, 0.8641975308641975, '[[18709  1082]\\n [   22   140]]']\n",
      "\n",
      "RandomForestClassifier(bootstrap=True,ccp_alpha=0.0,class_weight=None,criterion=gini,max_depth=3,max_features=0.3,max_leaf_nodes=None,max_samples=None,min_impurity_decrease=0.0,min_samples_leaf=1,min_samples_split=2,min_weight_fraction_leaf=0.0,monotonic_cst=None,n_estimators=1000,n_jobs=6,oob_score=False,random_state=32,verbose=0,warm_start=False)\n",
      "[0.2011577424023155, 0.11374795417348608, 0.86875, '[[18710  1083]\\n [   21   139]]']\n",
      "\n",
      "RandomForestClassifier(bootstrap=True,ccp_alpha=0.0,class_weight=None,criterion=gini,max_depth=3,max_features=0.3,max_leaf_nodes=None,max_samples=None,min_impurity_decrease=0.0,min_samples_leaf=1,min_samples_split=2,min_weight_fraction_leaf=0.0,monotonic_cst=None,n_estimators=1500,n_jobs=6,oob_score=False,random_state=32,verbose=0,warm_start=False)\n",
      "[0.20216606498194944, 0.11456628477905073, 0.8588957055214724, '[[18708  1082]\\n [   23   140]]']\n",
      "\n",
      "RandomForestClassifier(bootstrap=True,ccp_alpha=0.0,class_weight=None,criterion=gini,max_depth=6,max_features=sqrt,max_leaf_nodes=None,max_samples=None,min_impurity_decrease=0.0,min_samples_leaf=1,min_samples_split=2,min_weight_fraction_leaf=0.0,monotonic_cst=None,n_estimators=500,n_jobs=6,oob_score=False,random_state=32,verbose=0,warm_start=False)\n",
      "[0.019448946515397084, 0.009819967266775777, 1.0, '[[18731  1210]\\n [    0    12]]']\n",
      "\n",
      "RandomForestClassifier(bootstrap=True,ccp_alpha=0.0,class_weight=None,criterion=gini,max_depth=6,max_features=sqrt,max_leaf_nodes=None,max_samples=None,min_impurity_decrease=0.0,min_samples_leaf=1,min_samples_split=2,min_weight_fraction_leaf=0.0,monotonic_cst=None,n_estimators=1000,n_jobs=6,oob_score=False,random_state=32,verbose=0,warm_start=False)\n",
      "[0.021052631578947368, 0.010638297872340425, 1.0, '[[18731  1209]\\n [    0    13]]']\n",
      "\n",
      "RandomForestClassifier(bootstrap=True,ccp_alpha=0.0,class_weight=None,criterion=gini,max_depth=6,max_features=sqrt,max_leaf_nodes=None,max_samples=None,min_impurity_decrease=0.0,min_samples_leaf=1,min_samples_split=2,min_weight_fraction_leaf=0.0,monotonic_cst=None,n_estimators=1500,n_jobs=6,oob_score=False,random_state=32,verbose=0,warm_start=False)\n",
      "[0.017842660178426603, 0.00900163666121113, 1.0, '[[18731  1211]\\n [    0    11]]']\n",
      "\n",
      "CatBoostClassifier(iterations=500,silent=True,task_type=GPU,random_state=32)\n",
      "[0.5828220858895705, 0.4664484451718494, 0.776566757493188, '[[18567   652]\\n [  164   570]]']\n",
      "\n",
      "CatBoostClassifier(iterations=1000,silent=True,task_type=GPU,random_state=32)\n",
      "[0.5790281329923274, 0.46317512274959083, 0.772169167803547, '[[18564   656]\\n [  167   566]]']\n",
      "\n",
      "CatBoostClassifier(iterations=1500,silent=True,task_type=GPU,random_state=32)\n",
      "[0.5839714139867279, 0.46808510638297873, 0.7761194029850746, '[[18566   650]\\n [  165   572]]']\n",
      "\n",
      "CatBoostClassifier(iterations=2000,silent=True,task_type=GPU,random_state=32)\n",
      "[0.581799591002045, 0.4656301145662848, 0.7752043596730245, '[[18566   653]\\n [  165   569]]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#feature names\n",
    "feat_names = [col for col in encoded_train_df.columns if (col != TARGET_NAME)]\n",
    "\n",
    "\n",
    "models = [ LogisticRegression(C=1.0, class_weight='balanced', max_iter=200, n_jobs = N_JOBS, random_state=32), #strong regularization, class balancing\n",
    "          LogisticRegression(C=1.0, class_weight='balanced', max_iter=500, n_jobs = N_JOBS, random_state=32), \n",
    "          LogisticRegression(C=1.0, class_weight='balanced', max_iter=1000, n_jobs = N_JOBS, random_state=32), \n",
    "          LogisticRegression(C=0.1, class_weight='balanced', max_iter=200, n_jobs = N_JOBS, random_state=32), #weak regularization, class balancing\n",
    "          LogisticRegression(C=0.1, class_weight='balanced', max_iter=500, n_jobs = N_JOBS, random_state=32), \n",
    "          LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000, n_jobs = N_JOBS, random_state=32), \n",
    "\n",
    "          DecisionTreeClassifier(max_depth=4, class_weight='balanced', random_state=32),\n",
    "          DecisionTreeClassifier(max_depth=7, class_weight='balanced', random_state=32),\n",
    "          DecisionTreeClassifier(max_depth=10, class_weight='balanced', random_state=32),\n",
    "          DecisionTreeClassifier(max_depth=13, class_weight='balanced', random_state=32),\n",
    "\n",
    "          RandomForestClassifier(n_estimators=500, max_depth=3, max_features=0.3, n_jobs=N_JOBS, random_state=32), #try not very deep trees\n",
    "          RandomForestClassifier(n_estimators=1000, max_depth=3, max_features=0.3, n_jobs=N_JOBS, random_state=32),\n",
    "          RandomForestClassifier(n_estimators=1500, max_depth=3, max_features=0.3, n_jobs=N_JOBS, random_state=32),\n",
    "          RandomForestClassifier(n_estimators=500, max_depth=6, n_jobs=N_JOBS, random_state=32), #try deeper trees\n",
    "          RandomForestClassifier(n_estimators=1000, max_depth=6, n_jobs=N_JOBS, random_state=32),\n",
    "          RandomForestClassifier(n_estimators=1500, max_depth=6, n_jobs=N_JOBS, random_state=32),\n",
    "    \n",
    "          CatBoostClassifier(iterations = 500, task_type = 'GPU' if GPU else 'CPU', silent = True, random_state=32),\n",
    "          CatBoostClassifier(iterations = 1000, task_type = 'GPU' if GPU else 'CPU', silent = True,random_state=32),\n",
    "          CatBoostClassifier(iterations = 1500, task_type = 'GPU' if GPU else 'CPU', silent = True,random_state=32),\n",
    "          CatBoostClassifier(iterations = 2000, task_type = 'GPU' if GPU else 'CPU',  silent = True, random_state=32),\n",
    "         \n",
    "         \n",
    "         ]\n",
    "\n",
    "exp_name_base = 'baselines'\n",
    "\n",
    "#folder to store artifacts\n",
    "if not os.path.isdir(MODELS_PATH + exp_name_base):\n",
    "    os.mkdir(MODELS_PATH + exp_name_base)\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    model.fit(encoded_train_df[feat_names], encoded_train_df[TARGET_NAME])\n",
    "\n",
    "    preds = model.predict(encoded_val_df[feat_names])\n",
    "    metrics = evaluate_model(preds, encoded_val_df[TARGET_NAME])\n",
    "    model_name = get_model_string(model)\n",
    "    \n",
    "    log_model(exp_name_base, model_name,\n",
    "             MODELS_PATH + MODELS_LOG_FNAME,\n",
    "             metrics)\n",
    "\n",
    "    save_model_pickle(model, MODELS_PATH + f'{exp_name_base}/' + model_name[:100] + '.pkl')\n",
    "    print(model_name)\n",
    "    print(metrics)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "816f3b61-5ab0-4c90-9eb2-33161b720593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662d756-6862-4da8-be73-65bcc08c6066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
